/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: please do not edit it. Instead, edit the
// BAML files and re-generate this code using: baml-cli generate
// You can install baml-cli with:
//  $ npm install @boundaryml/baml
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code

const fileMap = {
  
  "clients.baml": "// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n\nclient<llm> Llama3 {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2\"\n  }\n}\n\nclient<llm> Gemma3 {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"Gemma3\"\n  }\n}\n\nclient<llm> Deepseek {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"deepseek-r1:1.5b\"\n    think false\n    stream false\n  }\n}\n\nclient<llm> Qwen3 {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"qwen3:0.6b\"\n    think false\n    stream false\n  }\n}\n",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript/react\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.203.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  randomNumber int\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  client \"Qwen3\"\n  prompt #\"\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n\n    \"#\n  }\n}\n",
}
export const getBamlFiles = () => {
    return fileMap;
}